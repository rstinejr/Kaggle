Kaggle Completition Assignment, Predictive Analytics

This is Bob Stine's submission for the Predictive Analytics assignment to complete
a problem from the Kaggle competition.

PART 1: PROBLEM DESCRIPTION

I chose the "Prudential Life Insurance Assessment" problem, described here:

https://www.kaggle.com/c/prudential-life-insurance-assessment/data

This problem is to develop an automated method to evaluate risk of granting 
life insurance to applicants, given values such as age, weight, health history,
and employment history.  Risk is an ordinal measure of 8 levels.

PART 2: INITIAL ANALYSIS APPROACH

The first step is to analyze the data.  Some fields are integral, others are
categorical. The initial steps will be:

1. review the fields and their values.
2. select fields to use in model, and identity those which are "nominal" 
   (i.e., labels rather than integer or real values).

After selecting and preparing the fields to use as input, use R to:

1. Partition the data into a training set and a test set.
2. Train a decision tree to predict accelerated claim approval,
   using the training set.
3. Observe results when the tree is applied to the test set.

PART 3: INITIAL SOLUTION

Before analysis, as a sanity checkon the partition between training and test data, I computed the mean of record ID, which in
the file is a strictly increasing integer.  The mean is close to equal in the two subsets:

    Training set extracted, mean ID of training set:  39605.79 
                            mean ID of test set    :  39408.64 
 
A decision tree with all columns would be intractible, so the first job was to identify the factors with high
predictive power.  To do this, I used the R "randomForest" function
to model batches of factors, and then applied the "importance" function to the model.  E.g.,

    fol <- formula(Response ~ Product_Info_1 + Product_Info_2 + Product_Info_3 + Product_Info_4
                            + Product_Info_5 + Product_Info_6 + Product_Info_7)
    prod_rf = randomForest(fol1, train)
    cat("\nimportance of Product_Info fields:\n")
    importance(prod_rf)

"Importance" is IncNodePurity, which according to this web article, 
jhttp://stats.stackexchange.com/questions/162465/in-a-random-forest-is-larger-incmse-better-or-worse

    "...relates to the loss function which by best splits are chosen. The loss function is mse for
     regression and gini-impurity for classification. More useful variables achieve higher increases in
     node purities, that is to find a split which has a high inter node 'variance' and a small intra node
     'variance'."

Results for the 10 groups of attributes were as follows:

	Importance of Group 1, Product_Info:
				   IncNodePurity
	Product_Info_1      196.1271
	Product_Info_2    10558.5313
	Product_Info_3     2302.8496
	Product_Info_4    10290.4014
	Product_Info_5      134.1557
	Product_Info_6      376.6041
	Product_Info_7      225.4159

	Importance, Group 2, age/wt/BMI:
			IncNodePurity
	Ins_Age      23058.66
	Ht            8458.03
	Wt           24801.23
	BMI          29424.46

	Importance of Group 3, Employment_Info:
					  IncNodePurity
	Employment_Info_1      6260.453
	Employment_Info_2      2958.396
	Employment_Info_3       988.157
	Employment_Info_4      3207.965
	Employment_Info_5      1219.497
	Employment_Info_6      4699.130

	Importance of Group 4, InsuredInfo:
				  IncNodePurity
	InsuredInfo_1     934.68628
	InsuredInfo_2     488.69328
	InsuredInfo_3     556.43612
	InsuredInfo_4      75.88892
	InsuredInfo_5    1125.44147
	InsuredInfo_6    2665.51316
	InsuredInfo_7     947.13608

	Importance of Group 5, Insurance_History:
						IncNodePurity
	Insurance_History_1      99.29961
	Insurance_History_2    1198.24286
	Insurance_History_3       0.00000
	Insurance_History_4     232.20398
	Insurance_History_5    1927.89421
	Insurance_History_7     173.63743
	Insurance_History_8     135.81395
	Insurance_History_9      74.96934

	Importance of Group 6, Family_Hist:
				  IncNodePurity
	Family_Hist_1      166.7946
	Family_Hist_2      525.9247
	Family_Hist_4      538.8467

	Importance of Group 7, Medical_History 1 - 9:
					  IncNodePurity
	Medical_History_1     7463.0620
	Medical_History_2     7479.6934
	Medical_History_3      814.3148
	Medical_History_4     7988.8806
	Medical_History_5      780.8322
	Medical_History_6     3331.1185
	Medical_History_7     1165.5608
	Medical_History_8     1216.0896
	Medical_History_9     1035.9542

	Importance of Group 8, Medical_History 10 - 19:
					   IncNodePurity
	Medical_History_10     69.087630
	Medical_History_11      5.663649
	Medical_History_12     25.558728
	Medical_History_13     28.622411
	Medical_History_14     56.268941
	Medical_History_15    126.413603
	Medical_History_16     18.455467
	Medical_History_17     16.133904
	Medical_History_18     26.935298
	Medical_History_19      8.193505

	Importance of Group 9, Medical_History 20 - 29:
					   IncNodePurity
	Medical_History_20      33.00462
	Medical_History_21      85.20223
	Medical_History_22      64.71012
	Medical_History_23     606.13865
	Medical_History_24     716.35800
	Medical_History_25      74.45694
	Medical_History_26      58.70741
	Medical_History_27      38.92482
	Medical_History_28     217.35366
	Medical_History_29     117.91429

	Importance of Group 10, Medical_History 30 - 41:
					   IncNodePurity
	Medical_History_30      27.28178
	Medical_History_31      67.73177
	Medical_History_32     243.94111
	Medical_History_33      28.87121
	Medical_History_34      37.99684
	Medical_History_35      10.85989
	Medical_History_36      47.72478
	Medical_History_37      20.26102
	Medical_History_38      18.88860
	Medical_History_39      94.17750
	Medical_History_40     163.84813
	Medical_History_41      32.03202

DATA CLEAN-UP NOTES:

There are misssing values in Group 3, Employment Info, Group 5, Insurance History 1- 7, Group 6,
Family History 1 - 5, Group 7, Medical History 1 - 9, Group 9, Medical History 20 - 29, and 
Group 10, Medical History 30 - 41.  Since randomForest cannot handle missing values in the
predictor attribues, I elected to omit these values.

Another issue I encountered was that when I attempted to create a random forest for the Family
History attributes, R  generated the following error:

    The response has five or fewer unique values.  Are you sure you want to do regression?
    Execution halted

To work around this issue, I removed factor Family_Hist_3 and Family_Hist 5 from the model.

The problem description sahs that Medical_History_2 is a nominal value. It has, however, 579 distinct
values. As a result, the randomTree function would not accept it when declared "as.factor".


FACTORS FOR MODEL

Examining the IncNodePurity values by eye, there appears to be a natural break at 900, which
gives us the following attributes:

	Attribute           IncNodePurity 
    ---------           -------------
	Product_Info_2      10558.5313
	Product_Info_3       2302.8496
	Product_Info_4      10290.4014
	Ins_Age             23058.66
	Ht                   8458.03
	Wt                  24801.23
	BMI                 29424.46
	Employment_Info_1    6260.453
	Employment_Info_2    2958.396
	Employment_Info_3     988.157
	Employment_Info_4    3207.965
	Employment_Info_5    1219.497
	Employment_Info_6    4699.130
	InsuredInfo_1         934.68628
	InsuredInfo_5        1125.44147
	InsuredInfo_6        2665.51316
	InsuredInfo_7         947.13608
	Insurance_History_2  1198.24286
	Insurance_History_5  1927.89421
	Medical_History_1    7463.0620
	Medical_History_2    7479.6934
	Medical_History_4    7988.8806
	Medical_History_6    3331.1185

The IncNodePUrity of a model with the above inputs is:

	Importance of high-impact factors:
						IncNodePurity
	Product_Info_2          5135.2892
	Product_Info_3           467.1862
	Product_Info_4          3577.0040
	Ins_Age                 5391.0110
	Ht                      2871.4668
	Wt                      6754.0585
	BMI                    10182.5628
	Employment_Info_1       3963.6542
	Employment_Info_2       2480.6412
	Employment_Info_3        316.5980
	Employment_Info_4       1461.5074
	Employment_Info_5        227.9790
	Employment_Info_6       3195.4737
	InsuredInfo_1            543.0781
	InsuredInfo_5            125.6827
	InsuredInfo_6            573.5616
	InsuredInfo_7            183.3555
	Insurance_History_2      557.5082
	Insurance_History_5     3564.3698
	Medical_History_1       3611.7122
	Medical_History_2       3422.6708
	Medical_History_4       2195.6901
	Medical_History_6        653.7544

Again, there appears to be a big fall-off in IncNodePurity after 1000, so we reduce the 
model to the following, of which all appear to contribute significantly to error reduction:

	Importance of reduced set of high-impact factors:
						IncNodePurity
	Product_Info_2           5283.072
	Product_Info_4           3797.586
	Ins_Age                  5596.023
	Ht                       3181.296
	Wt                       7139.026
	BMI                     10487.495
	Employment_Info_1        4307.644
	Employment_Info_2        2763.658
	Employment_Info_4        1560.891
	Employment_Info_6        3448.129
	Insurance_History_5      3827.292
	Medical_History_1        3879.575
	Medical_History_2        3710.799
	Medical_History_4        2195.531


PART 4: ANALYSIS OF INITIAL SOLUTION

Using R funcion "rpart" to create a regression tree with the above model yielded a surprise: only 3 factors
played a role:

	Regression tree for model:
	n= 29690 

	node), split, n, deviance, yval
		  * denotes terminal node

	 1) root 29690 179341.50 5.635736  
	   2) BMI>=0.5365671 7119  32206.88 4.203259  
		 4) BMI>=0.7229444 1144   3154.49 2.720280 *
		 5) BMI< 0.7229444 5975  26054.77 4.487197 *
	   3) BMI< 0.5365671 22571 127919.00 6.087546  
		 6) Medical_History_4=1 7278  41538.07 5.170239 *
		 7) Medical_History_4=2 15293  77342.37 6.524096  
		  14) Ins_Age>=0.5298507 4084  27419.13 5.657933 *
		  15) Ins_Age< 0.5298507 11209  45742.91 6.839682 *

This suggests that yet another factor pruning would be in order, 
which would leave BMI, Medical_History_4, and Ins_Age


PART 5: REVISED SOLUTION AND ANALYSIS

In the revised solution the IncNodePurity value for all remaining factors is large.
See below:

	Importance of age, BMI, and Medical History 4:
					  IncNodePurity
	Ins_Age                6461.032
	BMI                   20720.150
	Medical_History_4      9279.348

	Regression tree for model:
	n= 29690 

	node), split, n, deviance, yval
		  * denotes terminal node

	 1) root 29690 179341.50 5.635736  
	   2) BMI>=0.5365671 7119  32206.88 4.203259  
		 4) BMI>=0.7229444 1144   3154.49 2.720280 *
		 5) BMI< 0.7229444 5975  26054.77 4.487197 *
	   3) BMI< 0.5365671 22571 127919.00 6.087546  
		 6) Medical_History_4=1 7278  41538.07 5.170239 *
		 7) Medical_History_4=2 15293  77342.37 6.524096  
		  14) Ins_Age>=0.5298507 4084  27419.13 5.657933 *
		  15) Ins_Age< 0.5298507 11209  45742.91 6.839682 *

Using R's "predict" function to evaluate performace, we see that the performance
against the test set is comparable to the performance against the training set:

	Prediction table from training data:
					  true
	pred                  1    2    3    4    5    6    7    8
	  2.72027972027972  246  557   14    0  259   64    1    3
	  4.48719665271966  832  910  188   43 1444 1806  649  103
	  5.17023907666941  899  770  240  618  311 2141  795 1504
	  5.6579333986288   550  467    6   11  274  625  767 1384
	  6.83968239807298  550  638   21   56  411 1015 1736 6782

	Prediction table from test data:
					  true
	pred                  1    2    3    4    5    6    7    8
	  2.72027972027972  290  504   27    0  261   66    2    1
	  4.48719665271966  795  893  227   31 1478 1852  733  133
	  5.17023907666941  909  744  272  601  311 2080  836 1512
	  5.6579333986288   549  472    1    5  282  613  789 1368
	  6.83968239807298  587  597   17   63  401  971 1719 6699
