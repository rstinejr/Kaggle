Kaggle Completition Assignment, Predictive Analytics

This is Bob Stine's submission for the Predictive Analytics assignment to complete
a problem from the Kaggle competition.

PART 1: PROBLEM DESCRIPTION

I chose the "Prudential Life Insurance Assessment" problem, described here:

https://www.kaggle.com/c/prudential-life-insurance-assessment/data

This problem is to develop an automated method to evaluate risk of granting 
life insurance to applicants, given values such as age, weight, health history,
and employment history.  Risk is an ordinal measure of 8 levels.

PART 2: INITIAL ANALYSIS APPROACH

The first step is to analyze the data.  Some fields are integral, others are
categorical. The initial steps will be:

1. review the fields and their values.
2. select fields to use in model, and identity those which are "nominal" 
   (i.e., labels rather than integer or real values).

After selecting and preparing the fields to use as input, use R to:

1. Partition the data into a training set and a test set.
2. Train a decision tree to predict accelerated claim approval,
   using the training set.
3. Observe results when the tree is applied to the test set.

PART 3: INITIAL SOLUTION

The first job was to identify the factors with high predictive power.  To do this, I used the R "randomForest" function
to model batches of factors, and then applied the "importance" function to the model.  E.g.,

    fol <- formula(Response ~ Product_Info_1 + Product_Info_2 + Product_Info_3 + Product_Info_4
                            + Product_Info_5 + Product_Info_6 + Product_Info_7)
    prod_rf = randomForest(fol1, train)
    cat("\nimportance of Product_Info fields:\n")
    importance(prod_rf)

Results were as follows:
 

Before analysis, as a sanity checkon the partition between training and test data, I computed the mean of record ID, which in
the file is a strictly increasing integer.  The mean is close to equal in the two subsets:

    Training set extracted, mean ID of training set:  39605.79 
                            mean ID of test set    :  39408.64 
 
A decision tree with all columns would be intractible.  I decided by pick factors by running initial models with 
only certain categories of factors, to see whether they could create a decision tree that identified risk.


Group 1: Product ID.  

The insurance products are in 7 different categories, ProductInfo_1 - 7.  All but ProductInfo_4 are discrete, nominal values,
so I redefined them "as.factor" before building the model.  The R function "rpart" generated a decision tree based only
on ProductInfo_4, with one branch selecting Response 5, the other selecting Response 8. 

Group 2: Body Attributes

This group contains the attributes Age, height, weight, BMI.

PART 4: ANALYSIS OF INITIAL SOLUTION


This effect is probably because within the 8 result categories, the distribution is very uneven - the majority 
are category 8.  Several of the nominal fields also have a preponderance of a particular value, rather than an
even distribution.

In response, I adopted an alternative method: identify candidate factors by generating a random forest and then applying the 
"importance" function to it.

The results were as follows:

    Importance of Group 1, Product_Info fields:
                   IncNodePurity
    Product_Info_1      196.1271
    Product_Info_2    10558.5313
    Product_Info_3     2302.8496
    Product_Info_4    10290.4014
    Product_Info_5      134.1557
    Product_Info_6      376.6041
    Product_Info_7      225.4159


There are misssing values in Group 3, Employment Info, Group 5, Insurance History 1- 7, Group 6,
Family History 1 - 5, and Group 7, Medical_History 1 - 9. 

Since randomForest cannot handle missing values in the predictor attribues, I elected to omit these values.

When I attempted to create a random forest for the Family History attributes, R  generated the following error:

    The response has five or fewer unique values.  Are you sure you want to do regression?
    Execution halted

To work around this issue, I removed factor Family_Hist_3 and Family_Hist 5 from the model.

PART 5: REVISED SOLUTION AND ANALYSIS

Of the parameters rated "important, there appears to be a natrual break between those with values a
value over 1000 and those that are not rated or are rated less than 1000.  With this critereon, our
new model will include the following input:

    Product_Info_2
    Product_Info_3
    Product_Info_4
    Ins_Age
    Employment_Info_1
    InsuredInfo_1
    Family_Hist_1
    Medical_History_1
    Medical_History_20
    Medical_History_30

Unexpectedly, running "rpart" again resulted in a degenerate decision tree (i.e., no branches, just pick 8).  The
measure of importance from report forest only picked Product_info_2:

    Decision tree for revised model:
    n= 29690 

    node), split, n, loss, yval, (yprob)
          * denotes terminal node

    1) root 29690 19914 8 (0.1 0.11 0.016 0.025 0.091 0.19 0.13 0.33) *

    Importance in revised model
                   IncNodePurity
    Product_Info_2      12495.77

FURTHER ANALYSIS

Perhaps the results are defuse because I have treated "risk" categories as unrelated, when in fact they are an
ordinal relation, from highest risk (1) to lowest (8).  In an attempt to get some predictive power from the
data, I reran the "rpart" function as a regression, rather than a classification.  The training results were
as follows:


